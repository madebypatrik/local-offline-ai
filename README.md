# ðŸ§  local-offline-ai

> A local offline AI stack â€” no subscriptions, no internet, your data never leaves your machine

![Ollama](https://img.shields.io/badge/Ollama-local%20AI-black)
![macOS](https://img.shields.io/badge/macOS-Apple%20Silicon%20%7C%20Intel-lightgrey)
![License](https://img.shields.io/badge/license-ISC-yellow)
![Privacy](https://img.shields.io/badge/privacy-100%25%20offline-green)

---

## What is this?

This is a setup guide and toolkit for running AI **completely on your own computer** â€” no account required, no monthly fee, no internet connection needed once set up.

Your conversations, code, and documents **never leave your machine.**

---

## What can you do with it?

- ðŸ’¬ Chat with an AI assistant â€” like ChatGPT but fully private
- ðŸ‘¨â€ðŸ’» Get coding help and suggestions directly in VS Code
- âœï¸ Ask questions, summarize text, brainstorm ideas
- ðŸ”’ Work with sensitive documents without worrying about data leaks
- âœˆï¸ Use AI on a plane, in a cafÃ©, anywhere â€” no Wi-Fi needed

---

## What you need

- A Mac (Apple Silicon or Intel)
- [Homebrew](https://brew.sh) â€” a free tool for installing software on Mac
- [VS Code](https://code.visualstudio.com) â€” free code editor (optional, for coding features)
- ~5 GB of free disk space per AI model

---

## Setup â€” step by step

### 1. Install Homebrew (if you don't have it)
Open **Terminal** and paste:
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

### 2. Install Ollama
```bash
brew install ollama
```

### 3. Start the AI server
```bash
ollama serve
```
Keep this terminal window open â€” it runs the AI engine in the background.

### 4. Download an AI model
Open a **new** terminal window and run one of these:

```bash
# Great for coding
ollama pull deepseek-coder:6.7b

# Great for general chat, writing, and questions
ollama pull llama3
```

### 5. Start chatting
```bash
ollama run llama3
```
Type your message and press Enter. Type `/bye` to exit.

---

## Using the helper script

This repo includes `ollama.sh` â€” a small script with shortcuts so you don't have to remember commands.

```bash
# See all commands and cheat sheet
./ollama.sh

# Start the server
./ollama.sh serve

# Chat with the default model (deepseek-coder:6.7b)
./ollama.sh run

# Chat with a specific model
./ollama.sh run llama3

# See what models you have downloaded
./ollama.sh list
```

---

## VS Code integration (optional)

If you use VS Code, you can get AI suggestions and chat right inside your editor for free.

1. Open VS Code
2. Install the **Continue** extension (search `Continue.continue` in the Extensions panel)
3. In Continue settings, set the provider to **Ollama** and model to `deepseek-coder:6.7b`
4. That's it â€” AI tab-complete and chat work entirely offline

> To disable telemetry in Continue, add this to your VS Code settings:
> `"continue.telemetryEnabled": false`

---

## AI models included

See [TOOLS.md](./TOOLS.md) for full details on every tool and model â€” including company, country, open-source license, privacy rating, and popularity.

| Model | Best for | Size |
|---|---|---|
| `deepseek-coder:6.7b` | Code, programming help | ~3.8 GB |
| `llama3` | Chat, writing, questions | ~4.7 GB |

---

## Privacy & Safety

| | |
|---|---|
| Data sent to the cloud | **Never** |
| Internet required after setup | **No** |
| Account or API key needed | **No** |
| Subscription or cost | **Free** |
| Your files/code shared | **Never** |

All models run locally using [Ollama](https://ollama.com). Nothing is logged or transmitted.

---

## Troubleshooting

**`ollama: command not found`**
â†’ Make sure Homebrew installed correctly and run `brew install ollama` again.

**Model is slow**
â†’ Larger models need more RAM. Try a smaller model like `phi3:mini` (~2 GB).

**`ollama serve` port already in use**
â†’ Ollama might already be running. Check with `ollama ps`.

**Continue extension not connecting**
â†’ Make sure `ollama serve` is running in a terminal first.

---

## License

ISC Â© [@madebypatrik](https://github.com/madebypatrik)
