#!/usr/bin/env bash
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# local-offline-ai â€” one-command setup
# github.com/madebypatrik/local-offline-ai
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

set -e

BOLD="\033[1m"
GREEN="\033[0;32m"
YELLOW="\033[0;33m"
RESET="\033[0m"

print()  { echo -e "${BOLD}$1${RESET}"; }
ok()     { echo -e "${GREEN}âœ” $1${RESET}"; }
info()   { echo -e "${YELLOW}â†’ $1${RESET}"; }

echo ""
print "ðŸ§  local-offline-ai setup"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo "This will install Ollama and download"
echo "your first AI model â€” fully offline."
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""

# â”€â”€ 1. Install Ollama â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if command -v ollama &>/dev/null; then
  ok "Ollama is already installed"
else
  print "Step 1/3 â€” Installing Ollama..."

  if command -v brew &>/dev/null; then
    brew install ollama
  else
    # Download the Mac app and open it for the user
    info "Downloading Ollama..."
    TMP=$(mktemp -d)
    curl -L --progress-bar "https://ollama.com/download/Ollama-darwin.zip" -o "$TMP/Ollama.zip"
    info "Unpacking..."
    unzip -q "$TMP/Ollama.zip" -d "$TMP"
    info "Moving Ollama to Applications..."
    cp -r "$TMP/Ollama.app" /Applications/Ollama.app
    rm -rf "$TMP"
    info "Launching Ollama..."
    open /Applications/Ollama.app
    # Wait for the server to start
    echo "Waiting for Ollama to start..."
    for i in {1..20}; do
      if curl -s http://localhost:11434 &>/dev/null; then break; fi
      sleep 1
    done
  fi

  ok "Ollama installed"
fi

# â”€â”€ 2. Start Ollama server if not already running â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print "Step 2/3 â€” Starting Ollama server..."

if curl -s http://localhost:11434 &>/dev/null; then
  ok "Ollama server is already running"
else
  ollama serve &>/dev/null &
  echo "Waiting for server to start..."
  for i in {1..15}; do
    if curl -s http://localhost:11434 &>/dev/null; then break; fi
    sleep 1
  done
  ok "Ollama server started"
fi

# â”€â”€ 3. Download default model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DEFAULT_MODEL="deepseek-coder:6.7b"

print "Step 3/3 â€” Downloading AI model ($DEFAULT_MODEL)..."
info "This is a one-time download (~3.8 GB). Go grab a coffee â˜•"
echo ""

ollama pull "$DEFAULT_MODEL"

ok "Model ready"

# â”€â”€ Done â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

echo ""
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
print "âœ… All done! Your local AI is ready."
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "  Start chatting now:"
echo ""
echo "    ollama run $DEFAULT_MODEL"
echo ""
echo "  Or use the helper script:"
echo ""
echo "    ./ollama.sh run"
echo ""
echo "  For more models and commands see:"
echo "    TOOLS.md and ollama.sh"
echo ""
